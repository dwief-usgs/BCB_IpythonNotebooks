{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code registers GAP HUCs into the Spatial Features Registry.  The code is the first run at this and will likely change as a more complete understanding of SFR needs and requirements are established.  There are many notes within the notebook that suggest improvements to the code and the SB structure that we can implement to help support a more fluid SFR process.\n",
    "\n",
    "# The code highlights how to post large sql queries to gc2.  It also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request as ur\n",
    "import geopandas as gpd\n",
    "import subprocess\n",
    "import requests\n",
    "from bis2 import gc2\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "#Delete\n",
    "hucDownload = 'https://usgs-gap-data.s3.amazonaws.com/Nat_Ranges_Ancillary/HUCs_lower48.gdb.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('HUCs_lower48.gdb.zip', <http.client.HTTPMessage at 0x1f70065b550>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: This code pulls data down to local disk from direct call to file on amazon s3, \n",
    "#it would be ideal to use ScienceBase to access the file of interest and also to do processing in memory (WebLink needs a meaninful title to make that consistently work), \n",
    "#also should look into using s3 libraries for python (http://boto3.readthedocs.io/en/latest/reference/services/s3.html)\n",
    "#Somewhere in this step we should let the data type and SB item \"drive\" the process.  For example determine appropriate \n",
    "#process to get file into geodataframe based on file type.\n",
    "\n",
    "#Direct HUC Download URL\n",
    "hucDownload ='https://usgs-gap-data.s3.amazonaws.com/Nat_Ranges_Ancillary/HUCs_lower48.gdb.zip'\n",
    "#Download GAP HUC12 file to local directory\n",
    "ur.urlretrieve(hucDownLoad, 'HUCs_lower48.gdb.zip')\n",
    "#In working directory unzips file\n",
    "subprocess.call(r'\"C:\\Program Files\\7-Zip\\7z.exe\" x ' + 'HUCs_lower48.gdb.zip' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create GeoDataFrame of GAP HUC12s, from unzipped file\n",
    "ghuc_gdf = gpd.read_file('HUCs_lower48.gdb', layer='Hucs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HUC12RNG</th>\n",
       "      <th>STATES</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>030902030300</td>\n",
       "      <td>FL</td>\n",
       "      <td>5.016106e+09</td>\n",
       "      <td>467346.993995</td>\n",
       "      <td>(POLYGON ((1505886.25 332166.5, 1508386.25 330...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121102080900</td>\n",
       "      <td>TX</td>\n",
       "      <td>1.114051e+09</td>\n",
       "      <td>287217.454218</td>\n",
       "      <td>(POLYGON ((-129847.9686999992 339565.593799999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121102080100</td>\n",
       "      <td>TX</td>\n",
       "      <td>5.572368e+08</td>\n",
       "      <td>213775.315108</td>\n",
       "      <td>(POLYGON ((-238270.4844000004 355676.25, -2382...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121102080600</td>\n",
       "      <td>TX</td>\n",
       "      <td>4.453596e+08</td>\n",
       "      <td>131953.362328</td>\n",
       "      <td>(POLYGON ((-179788.9061999992 355720.093799999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121102080700</td>\n",
       "      <td>TX</td>\n",
       "      <td>3.380938e+08</td>\n",
       "      <td>112037.983271</td>\n",
       "      <td>(POLYGON ((-149064.7186999992 366221.343799999...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       HUC12RNG STATES    Shape_Area   Shape_Length  \\\n",
       "0  030902030300     FL  5.016106e+09  467346.993995   \n",
       "1  121102080900     TX  1.114051e+09  287217.454218   \n",
       "2  121102080100     TX  5.572368e+08  213775.315108   \n",
       "3  121102080600     TX  4.453596e+08  131953.362328   \n",
       "4  121102080700     TX  3.380938e+08  112037.983271   \n",
       "\n",
       "                                            geometry  \n",
       "0  (POLYGON ((1505886.25 332166.5, 1508386.25 330...  \n",
       "1  (POLYGON ((-129847.9686999992 339565.593799999...  \n",
       "2  (POLYGON ((-238270.4844000004 355676.25, -2382...  \n",
       "3  (POLYGON ((-179788.9061999992 355720.093799999...  \n",
       "4  (POLYGON ((-149064.7186999992 366221.343799999...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghuc_gdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datum': 'NAD83',\n",
       " 'lat_0': 23,\n",
       " 'lat_1': 29.5,\n",
       " 'lat_2': 45.5,\n",
       " 'lon_0': -96,\n",
       " 'no_defs': True,\n",
       " 'proj': 'aea',\n",
       " 'units': 'm',\n",
       " 'x_0': 0,\n",
       " 'y_0': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View current coordinate system of file.  This is just a note and can get rid of this step.\n",
    "#Currently this spatial file does not have an official crs... something to improve in future iterations\n",
    "ghuc_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init': 'epsg:3857'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform coordinate system to web mercator\n",
    "ghuc_gdf = ghuc_gdf.to_crs({'init': 'epsg:3857'}) \n",
    "#Print new coordinate system to make sure transformation worked\n",
    "ghuc_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-d1ea1ca99475>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Verify Coordinate System is in web mercator (crs: 3857), if not transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mghuc_gdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'init'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'epsg:3857'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Coordinate System = Web Mercator, crs: 3857'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0moldCrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mghuc_gdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'init'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'init'"
     ]
    }
   ],
   "source": [
    "#Verify Coordinate System is in web mercator (crs: 3857), if not transform, This needs to be improved upon, right now throws an error\n",
    "if ghuc_gdf.crs['init'] == 'epsg:3857':\n",
    "    print ('Coordinate System = Web Mercator, crs: 3857')\n",
    "else:\n",
    "    oldCrs = ghuc_gdf.crs['init']\n",
    "    ghuc_gdf.to_crs({'init': 'epsg:3857'})\n",
    "    newCrs = ghuc_gdf.crs['init']\n",
    "    print ('Transformed Coordinate System to ' + newCrs + ' from ' + oldCrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the actions/targets for this particular instance\n",
    "thisRun = {}\n",
    "thisRun[\"instance\"] = \"DataDistillery\"\n",
    "thisRun[\"db\"] = \"BCB\"\n",
    "thisRun[\"baseURL\"] = gc2.sqlAPI(thisRun[\"instance\"],thisRun[\"db\"])\n",
    "thisRun[\"schema\"] = \"sfr\"\n",
    "thisRun[\"commitToDB\"] = False\n",
    "\n",
    "gc2Key = gc2.gc2Keys[\"datadistillery_bcb\"]\n",
    "SbUrl = 'https://www.sciencebase.gov/catalog/item/56d496eee4b015c306f17a42'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170102030802 failed with code: 502\n",
      "100401040303 failed with code: 502\n",
      "170102120601 failed with code: 502\n",
      "100302032203 failed with code: 502\n",
      "170102120805 failed with code: 502\n",
      "090203131309 failed with code: 502\n",
      "170103050301 failed with code: 502\n",
      "010100030101 failed with code: 502\n"
     ]
    }
   ],
   "source": [
    "#This step helps troubleshoot the upload of many large complex features, which can not be uploaded in one run due to timing out of server\n",
    "#Return GAP HUC records already in SFR into a list called lookup.  Lookup will be used to ensure we don't duplicate records\n",
    "\n",
    "q_recordToSearch = \"SELECT registration->'sourceLookup' as lookup FROM sfr.sfr_poly where ftype= 'https://www.sciencebase.gov/vocab/term/5898adfde4b050e6125b807f' \" \n",
    "recordToSearch = requests.get(thisRun[\"baseURL\"]+\"&q=\"+q_recordToSearch).json()\n",
    "p = recordToSearch['features']\n",
    "lookup = []\n",
    "for f in p:\n",
    "    string = f['properties']['lookup']\n",
    "    if string.startswith('\"') and string.endswith('\"'):\n",
    "        string = string[1:-1]  #For some reason these values are coming back with double quotes, this removes outer quote\n",
    "        lookup.append(string)\n",
    "lenList = len(lookup)\n",
    "cntExisting = 0 \n",
    "existingList = []\n",
    "    \n",
    "for row in ghuc_gdf.itertuples():\n",
    "    #This if statement checks to see if the HUC was already processed, if it was we just record that and move on. \n",
    "    if row.HUC12RNG in lookup:\n",
    "        cntExisting += 1\n",
    "        existingList.append({'existingHuc': row.HUC12RNG})\n",
    "    else:\n",
    "        #Register a spatial feature\n",
    "        #Uses the ScienceBase item and data brought into a geodataframe with crs 3857 to register an item\n",
    "\n",
    "        #def registerSFR (SbUrl, gdf):\n",
    "        poly = str(row.geometry)\n",
    "        title = ('GAP_HUC_' + row.HUC12RNG)\n",
    "        spat_cert = 'https://www.sciencebase.gov/vocab/term/5822257ee4b0b3d9add24304'\n",
    "        ftype = 'https://www.sciencebase.gov/vocab/term/5898adfde4b050e6125b807f'\n",
    "              \n",
    "        #Build JSON for Registration Information field in SFR \n",
    "        #We should bring in information from the SB item to help build this out, example is dataDevelopmentDate, \n",
    "        thisRegistration = {}\n",
    "        thisRegistration[\"sourceName\"] = \"USGS Biogeographic Characterization Branch\"  #In Future Pull this from SB\n",
    "        thisRegistration[\"sourceInfoUrl\"] = SbUrl # In Future Pull this from SB\n",
    "        thisRegistration[\"sourceFileUrl\"] = hucDownload # In Future Pull this from SB\n",
    "        thisRegistration[\"registrationDate\"] = datetime.datetime.utcnow().isoformat() \n",
    "        thisRegistration[\"sourceLookup\"] = row.HUC12RNG\n",
    "        jd = json.dumps(thisRegistration)\n",
    "        \n",
    "        q = \"insert into sfr.sfr_poly(registration, spat_cert, title, ftype, the_geom) VALUES ('\" + jd + \"' ,'\"  + spat_cert + \"' ,'\" \\\n",
    "+ title + \"' ,'\" + ftype + \"', ST_GeomFromText('\" + poly + \"', 3857))\"\n",
    "        payload = \"q=%s&key=%s\"%(q,gc2Key)\n",
    "        url= gc2.baseURLs[\"sqlapi_datadistillery_bcb\"]\n",
    "        \n",
    "        try:\n",
    "            d=[]\n",
    "            r = requests.post(url,data=payload)\n",
    "            status = str(r.status_code)\n",
    "            if r.status_code == 200:\n",
    "                d.append({'status': status, 'id': str(row.HUC12RNG)})\n",
    "            else:\n",
    "                d.append({'status': status, 'id': str(row.HUC12RNG)})\n",
    "                print (str(row.HUC12RNG) + ' failed with code: ' + status)\n",
    "        except requests.exceptions.HTTPError:\n",
    "                d.append({'status': status, 'id': str(row.HUC12RNG)})\n",
    "                print (str(row.HUC12RNG) + ' failed with code: ' + status)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error that keeps coming up:\n",
    "ConnectionError: HTTPSConnectionPool(host='gc2.datadistillery.org', port=443): Max retries exceeded with url: /api/v1/sql/bcb (Caused by NewConnectionError('\n",
    " <requests.packages.urllib3.connection.VerifiedHTTPSConnection object at  0x00000234DBEEC588>: \n",
    " Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The below numbers should be the same when initially processing the file.  This could be built into an automated test later on.\n",
    "print ('Length of existing features list: ' + lenList)\n",
    "print ('Existing features that were not processed: ' + cntExisting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d)\n",
    "for index, row in df.iterrows():\n",
    "    if  row['status'] != '200':\n",
    "        print row['comid'], '           ', row['coordCnt'], '                       ', row['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gc2.datadistillery.org/api/v1/sql/bcb\n"
     ]
    }
   ],
   "source": [
    "print (url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FionaValueError",
     "evalue": "No dataset found at path '/vsizip//vsimem/f943e5efbec74f2caa8b60b2c54b11d9.zip' using drivers: *",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFionaValueError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-eea40def378d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mvsiz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/vsimem/{}.zip'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mgdal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileFromMemBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvsiz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmysock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCollection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvsiz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvsi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'zip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Hucs'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mgdf\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mgpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\dwieferich\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\fiona\\collection.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start (fiona/ogrext2.c:8570)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFionaValueError\u001b[0m: No dataset found at path '/vsizip//vsimem/f943e5efbec74f2caa8b60b2c54b11d9.zip' using drivers: *"
     ]
    }
   ],
   "source": [
    "#Testing for keeping zip gdb in memory.  \n",
    "#Code Failed when trying to view specific layer.\n",
    "\n",
    "import requests\n",
    "import uuid\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal\n",
    "import fiona\n",
    "\n",
    "#requests = requests.get()\n",
    "vsiz = '/vsimem/{}.zip'.format(uuid.uuid4().hex)\n",
    "gdal.FileFromMemBuffer(vsiz,bytes(mysock))\n",
    "with fiona.Collection(vsiz, vsi='zip', layer='Hucs') as f:\n",
    "    gdf =gpd.GeoDataFrame.from_features(f,crs=f.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82717, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghuc_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://geoportal.statistics.gov.uk/datasets/687f346f5023410ba86615655ff33ca9_3.zip')\n",
    "zf = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "zf.extractall(path=tempdir)\n",
    "counties = gpd.read_file(join(tempdir, 'Counties_and_Unitary_Authorities_December_2016_Super_Generalised_Clipped_Boundaries_in_England_and_Wales.shp'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
